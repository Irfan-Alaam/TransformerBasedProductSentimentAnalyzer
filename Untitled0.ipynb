{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Irfan-Alaam/TransformerBasedProductSentimentAnalyzer/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipO7QEDtHT1J",
        "outputId": "805af7e4-7c10-4a5f-fd09-fbb8f20a8457"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.12), please consider upgrading to the latest version (0.3.13).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/bittlingmayer/amazonreviews?dataset_version_number=7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 493M/493M [00:06<00:00, 79.1MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /root/.cache/kagglehub/datasets/bittlingmayer/amazonreviews/versions/7\n",
            "Loading small subset for testing...\n",
            "Vocab size: 20000\n",
            "Train shape: (60000, 100) (60000,)\n",
            "Test shape : (6000, 100) (6000,)\n"
          ]
        }
      ],
      "source": [
        "import os, bz2, re, string\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"bittlingmayer/amazonreviews\")\n",
        "print(\"Dataset downloaded to:\", path)\n",
        "\n",
        "train_file = os.path.join(path, \"train.ft.txt.bz2\")\n",
        "test_file  = os.path.join(path, \"test.ft.txt.bz2\")\n",
        "\n",
        "# Text cleaning + tokenizer\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"<br />\", \" \")\n",
        "    text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
        "    return text.split()\n",
        "\n",
        "# Load FastText format (label + review)\n",
        "def load_data(file, limit=None):\n",
        "    reviews, labels = [], []\n",
        "    with bz2.open(file, \"rt\", encoding=\"utf-8\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if limit and i >= limit: break\n",
        "            parts = line.strip().split(\" \", 1)\n",
        "            if len(parts) != 2: continue\n",
        "            label, review = parts\n",
        "            label = 0 if label == \"__label__1\" else 1\n",
        "            tokens = clean_text(review)\n",
        "            reviews.append(tokens)\n",
        "            labels.append(label)\n",
        "    return reviews, labels\n",
        "\n",
        "print(\"Loading small subset for testing...\")\n",
        "train_reviews, train_labels = load_data(train_file, limit=60000)\n",
        "test_reviews, test_labels   = load_data(test_file, limit=6000)\n",
        "\n",
        "# Build Vocabulary\n",
        "def build_vocab(token_lists, vocab_size=20000):\n",
        "    counter = Counter()\n",
        "    for tokens in token_lists:\n",
        "        counter.update(tokens)\n",
        "    vocab = {word: i+2 for i, (word, _) in enumerate(counter.most_common(vocab_size-2))}\n",
        "    vocab[\"<PAD>\"] = 0\n",
        "    vocab[\"<UNK>\"] = 1\n",
        "    return vocab\n",
        "#Here the counter counts how often each word appears in token_lists , picks top 19998 words(2 reserved for <PAD> and<UNK>) assign each word an integer ID starting from 2\n",
        "vocab = build_vocab(train_reviews, vocab_size=20000)\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "\n",
        "# Encode + Pad\n",
        "def encode_and_pad(tokens, vocab, max_len=100):\n",
        "    ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
        "    if len(ids) < max_len:\n",
        "        ids += [vocab[\"<PAD>\"]] * (max_len - len(ids))\n",
        "    else:\n",
        "        ids = ids[:max_len]\n",
        "    return ids\n",
        "\n",
        "X_train = np.array([encode_and_pad(t, vocab) for t in train_reviews], dtype=np.int32)\n",
        "X_test  = np.array([encode_and_pad(t, vocab) for t in test_reviews], dtype=np.int32)\n",
        "y_train = np.array(train_labels, dtype=np.int32)\n",
        "y_test  = np.array(test_labels, dtype=np.int32)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, y_train.shape)\n",
        "print(\"Test shape :\", X_test.shape, y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhhspcfXHdKA"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    if x.ndim == 1:\n",
        "        x = x - np.max(x)#keeps number stable\n",
        "        exps = np.exp(x)\n",
        "        return exps / np.sum(exps)\n",
        "    else:\n",
        "        x = x - np.max(x, axis=1, keepdims=True)\n",
        "        exps = np.exp(x)\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def layer_norm(x, eps=1e-6):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    var = np.var(x, axis=-1, keepdims=True)\n",
        "    return (x - mean) / np.sqrt(var + eps)\n",
        "\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    probs = softmax(logits)\n",
        "    batch_size = labels.shape[0]\n",
        "    log_likelihood = -np.log(probs[np.arange(batch_size), labels] + 1e-12)\n",
        "    return np.mean(log_likelihood)\n",
        "\n",
        "class Adam:\n",
        "    def __init__(self, params, lr=0.001, betas=(0.9,0.999), eps=1e-8):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "        self.betas = betas\n",
        "        self.eps = eps\n",
        "        self.m = [np.zeros_like(p) for p in params]\n",
        "        self.v = [np.zeros_like(p) for p in params]\n",
        "        self.t = 0\n",
        "    def step(self, grads):\n",
        "        self.t += 1\n",
        "        b1, b2 = self.betas\n",
        "        for i, (p, g) in enumerate(zip(self.params, grads)):\n",
        "            self.m[i] = b1 * self.m[i] + (1-b1)*g\n",
        "            self.v[i] = b2 * self.v[i] + (1-b2)*(g**2)\n",
        "            m_hat = self.m[i]/(1-b1**self.t)\n",
        "            v_hat = self.v[i]/(1-b2**self.t)\n",
        "            p -= self.lr * m_hat / (np.sqrt(v_hat)+self.eps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeV_yHhBHfOe"
      },
      "outputs": [],
      "source": [
        "# 1. (Token Embedding)\n",
        "class Embedding:\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        self.weight = np.random.randn(vocab_size, d_model) / np.sqrt(vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.weight[x]\n",
        "\n",
        "\n",
        "# 2. Positional Encoding (sinusoidal)\n",
        "def positional_encoding(seq_len, d_model):\n",
        "    pos = np.arange(seq_len)[:, np.newaxis]\n",
        "    i = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / d_model)\n",
        "    angle_rads = pos * angle_rates\n",
        "    pe = np.zeros((seq_len, d_model))\n",
        "    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])#sin function\n",
        "    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])#cos function\n",
        "    return pe\n",
        "\n",
        "# 3. Scaled Dot-Product Attention\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Q,K,V: (batch_size, seq_len, d_k)\n",
        "    mask: optional, same shape as attention logits\n",
        "    returns: output, attention_weights\n",
        "    \"\"\"\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = np.matmul(Q, K.transpose(0,2,1)) / np.sqrt(d_k)  # (batch, seq, seq)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = np.where(mask==0, -1e9, scores)\n",
        "\n",
        "    attn = softmax(scores)\n",
        "    output = np.matmul(attn, V)\n",
        "    return output, attn\n",
        "# 4. Multi-Head Attention\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        # weights for linear projections\n",
        "        self.Wq = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
        "        self.Wk = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
        "        self.Wv = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
        "        self.Wo = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "        return x.transpose(0,2,1,3)  # (batch, heads, seq_len, d_k)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        # x: (batch, heads, seq_len, d_k)\n",
        "        batch_size, heads, seq_len, d_k = x.shape\n",
        "        x = x.transpose(0,2,1,3).reshape(batch_size, seq_len, heads*d_k)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # linear projections\n",
        "        Q = np.matmul(x, self.Wq)\n",
        "        K = np.matmul(x, self.Wk)\n",
        "        V = np.matmul(x, self.Wv)\n",
        "\n",
        "        # split into heads\n",
        "        Q = self.split_heads(Q)\n",
        "        K = self.split_heads(K)\n",
        "        V = self.split_heads(V)\n",
        "\n",
        "        # compute attention per head\n",
        "        batch_size, heads, seq_len, d_k = Q.shape\n",
        "        out_heads = []\n",
        "        for h in range(heads):\n",
        "            out, _ = scaled_dot_product_attention(Q[:,h,:,:], K[:,h,:,:], V[:,h,:,:])\n",
        "            out_heads.append(out)\n",
        "        out_heads = np.stack(out_heads, axis=1)\n",
        "\n",
        "        # combine heads\n",
        "        out = self.combine_heads(out_heads)\n",
        "        out = np.matmul(out, self.Wo)\n",
        "        return out\n",
        "\n",
        "# 5. Feedforward Network\n",
        "class FeedForward:\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        self.W1 = np.random.randn(d_model, d_ff) / np.sqrt(d_model)\n",
        "        self.b1 = np.zeros(d_ff)\n",
        "        self.W2 = np.random.randn(d_ff, d_model) / np.sqrt(d_ff)\n",
        "        self.b2 = np.zeros(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = np.matmul(x, self.W1) + self.b1\n",
        "        x = np.maximum(0, x)  # ReLU\n",
        "        x = np.matmul(x, self.W2) + self.b2\n",
        "        return x\n",
        "\n",
        "# 6. Transformer Encoder Block\n",
        "class EncoderLayer:\n",
        "    def __init__(self, d_model, num_heads, d_ff):\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = FeedForward(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-head Attention + Residual + LayerNorm(Shortcut connection)\n",
        "        attn_out = self.mha.forward(x)\n",
        "        x = layer_norm(x + attn_out)\n",
        "\n",
        "        # Feedforward + Residual + LayerNorm(Shortcut connection)\n",
        "        ffn_out = self.ffn.forward(x)\n",
        "        x = layer_norm(x + ffn_out)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzZFk00cHhQx",
        "outputId": "525b2384-bf1e-4322-b99c-c9f515010245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits shape: (8, 2)\n",
            "Probabilities shape: (8, 2)\n",
            "Sample probabilities: [[0.61638124 0.38361876]\n",
            " [0.61638537 0.38361463]\n",
            " [0.61549842 0.38450158]\n",
            " [0.61563032 0.38436968]\n",
            " [0.61621955 0.38378045]\n",
            " [0.61590473 0.38409527]\n",
            " [0.61563877 0.38436123]\n",
            " [0.61528692 0.38471308]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1. Transformer Encoder (stacked layers)\n",
        "class TransformerEncoder:\n",
        "    def __init__(self, vocab_size, seq_len, d_model=64, num_heads=4, d_ff=128, num_layers=2, num_classes=2):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.num_layers = num_layers\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Embedding + Positional Encoding\n",
        "        self.embedding = Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(seq_len, d_model)\n",
        "\n",
        "        # Stack of Encoder layers\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)]\n",
        "\n",
        "        # Final classifier\n",
        "        self.W_out = np.random.randn(d_model, num_classes) / np.sqrt(d_model)\n",
        "        self.b_out = np.zeros(num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, seq_len) token indices\n",
        "        returns: logits (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Embedding + add positional encoding\n",
        "        x = self.embedding.forward(x)\n",
        "        x += self.pos_encoding[np.newaxis, :, :]\n",
        "\n",
        "        # Pass through N encoder layers\n",
        "        for layer in self.enc_layers:\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        # Pooling\n",
        "        x = np.mean(x, axis=1)  # (batch, d_model)\n",
        "\n",
        "        # Final classifier\n",
        "        logits = np.matmul(x, self.W_out) + self.b_out\n",
        "        return logits\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "seq_len = 100\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "d_ff = 128\n",
        "num_layers = 2\n",
        "num_classes = 2#number of output classes i.e 0=negative, 1=positive\n",
        "\n",
        "model = TransformerEncoder(vocab_size, seq_len, d_model, num_heads, d_ff, num_layers, num_classes)\n",
        "\n",
        "# Forward pass example\n",
        "batch_input = X_train[:8]\n",
        "logits = model.forward(batch_input)\n",
        "probs = softmax(logits)\n",
        "print(\"Logits shape:\", logits.shape)\n",
        "print(\"Probabilities shape:\", probs.shape)\n",
        "print(\"Sample probabilities:\", probs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcT-pY2fHjcH",
        "outputId": "6ab72d0c-3d87-4e5e-fc5b-a4e13376fc67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Loss: 0.5091 - Accuracy: 0.7665\n",
            "Epoch 2/3 - Loss: 0.2959 - Accuracy: 0.8905\n",
            "Epoch 3/3 - Loss: 0.2456 - Accuracy: 0.9086\n"
          ]
        }
      ],
      "source": [
        "# 1. Adam Optimizer for all parameters\n",
        "# Collect all trainable parameters\n",
        "def predict(model, x):\n",
        "    logits = model.forward(x)\n",
        "    probs = softmax(logits)\n",
        "    preds = np.argmax(probs, axis=1)\n",
        "    return preds, probs\n",
        "params = [model.embedding.weight, model.W_out, model.b_out]\n",
        "\n",
        "# Include encoder layer params\n",
        "for layer in model.enc_layers:\n",
        "    mha = layer.mha\n",
        "    params += [mha.Wq, mha.Wk, mha.Wv, mha.Wo]\n",
        "    ffn = layer.ffn\n",
        "    params += [ffn.W1, ffn.b1, ffn.W2, ffn.b2]\n",
        "#Adam optimizer updates all paramaters in one call\n",
        "optimizer = Adam(params, lr=0.001)\n",
        "\n",
        "# 2. Helper functions\n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(np.float32)\n",
        "\n",
        "# 3. Compute loss + gradients\n",
        "def compute_loss_and_grads_full(model, x_batch, y_batch):\n",
        "    \"\"\"\n",
        "    Forward + Backward pass including embeddings, encoder layers, and classifier.\n",
        "    For simplicity, we approximate gradients for encoder layers via identity (skip full MHA backprop).\n",
        "    \"\"\"\n",
        "\n",
        "    batch, seq_len = x_batch.shape\n",
        "    d_model = model.d_model\n",
        "\n",
        "    # ----------------- Forward pass -----------------\n",
        "    # Embedding + Positional encoding\n",
        "    x = model.embedding.forward(x_batch)  # (batch, seq_len, d_model)\n",
        "    x += model.pos_encoding[np.newaxis, :, :]\n",
        "\n",
        "    activations = [x]  # store input to each layer\n",
        "    for layer in model.enc_layers:\n",
        "        x = layer.forward(x)\n",
        "        activations.append(x)\n",
        "\n",
        "    # Pooling\n",
        "    pooled = np.mean(x, axis=1)  # (batch, d_model)\n",
        "\n",
        "    # Classifier\n",
        "    logits = np.matmul(pooled, model.W_out) + model.b_out\n",
        "    loss = cross_entropy_loss(logits, y_batch)\n",
        "\n",
        "    # ----------------- Backward pass -----------------\n",
        "    # Gradient w.r.t logits\n",
        "    probs = softmax(logits)\n",
        "    probs[np.arange(batch), y_batch] -= 1\n",
        "    probs /= batch\n",
        "\n",
        "    # Gradients for classifier\n",
        "    dW_out = np.matmul(pooled.T, probs)\n",
        "    db_out = np.sum(probs, axis=0)\n",
        "\n",
        "    # Gradient for pooled embeddings\n",
        "    dpooled = np.matmul(probs, model.W_out.T)  # (batch, d_model)\n",
        "    demb_seq = np.tile(dpooled[:, np.newaxis, :], (1, seq_len, 1)) / seq_len  # mean pooling\n",
        "\n",
        "    # ----------------- Backprop through last encoder layer FFN -----------------\n",
        "    layer = model.enc_layers[-1]\n",
        "    ffn = layer.ffn\n",
        "\n",
        "    # Flatten for matmul\n",
        "    x_input = activations[-2]  # input to this layer (batch, seq_len, d_model)\n",
        "    hidden = np.matmul(x_input, ffn.W1) + ffn.b1  # (batch, seq_len, d_ff)\n",
        "    hidden_flat = hidden.reshape(-1, ffn.W1.shape[1])  # (batch*seq_len, d_ff)\n",
        "    dx_flat = demb_seq.reshape(-1, d_model)  # (batch*seq_len, d_model)\n",
        "\n",
        "    # ReLU mask\n",
        "    relu_mask = (hidden_flat > 0)\n",
        "\n",
        "    # Gradients for W2, b2\n",
        "    dW2 = np.matmul(hidden_flat.T, dx_flat)\n",
        "    db2 = np.sum(dx_flat, axis=0)\n",
        "\n",
        "    # Gradient through ReLU and W1\n",
        "    dhidden = np.matmul(dx_flat, ffn.W2.T) * relu_mask\n",
        "    dW1 = np.matmul(x_input.reshape(-1, d_model).T, dhidden)\n",
        "    db1 = np.sum(dhidden, axis=0)\n",
        "\n",
        "    # Gradient for embedding (approx: backprop only through FFN of last layer)\n",
        "    dx_ffn = np.matmul(dhidden, ffn.W1.T)  # (batch*seq_len, d_model)\n",
        "    demb = dx_ffn.reshape(batch, seq_len, d_model) + demb_seq  # add pooled gradient\n",
        "\n",
        "    # Gradient w.r.t embedding weights\n",
        "    dEmbedding = np.zeros_like(model.embedding.weight)\n",
        "    for i in range(batch):\n",
        "        for j in range(seq_len):\n",
        "            token_idx = x_batch[i, j]\n",
        "            dEmbedding[token_idx] += demb[i, j]\n",
        "\n",
        "    grads = [dEmbedding, dW_out, db_out]\n",
        "\n",
        "    return loss, grads\n",
        "\n",
        "# 4. Training Loop\n",
        "batch_size = 32\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    perm = np.random.permutation(len(X_train))\n",
        "    X_train_shuffled = X_train[perm]\n",
        "    y_train_shuffled = y_train[perm]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(0, len(X_train), batch_size):\n",
        "        x_batch = X_train_shuffled[i:i+batch_size]\n",
        "        y_batch = y_train_shuffled[i:i+batch_size]\n",
        "\n",
        "        loss, grads = compute_loss_and_grads_full(model, x_batch, y_batch)\n",
        "        optimizer.step(grads)\n",
        "        epoch_loss += loss * len(x_batch)\n",
        "\n",
        "        # Accuracy\n",
        "        preds, _ = predict(model, x_batch)\n",
        "        correct += np.sum(preds == y_batch)\n",
        "\n",
        "    epoch_loss /= len(X_train)\n",
        "    accuracy = correct / len(X_train)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpYDvuaMHl19",
        "outputId": "66b3d1c4-8257-4cde-c293-2f0b1cdd85f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved to transformer_checkpoint.pkl\n",
            "Test Accuracy: 0.8845\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "def save_checkpoint(model, vocab, path=\"transformer_checkpoint.pkl\"):\n",
        "    checkpoint = {\n",
        "        \"model_params\": {\n",
        "            \"embedding_weight\": model.embedding.weight,\n",
        "            \"enc_Wq\": [layer.mha.Wq for layer in model.enc_layers],\n",
        "            \"enc_Wk\": [layer.mha.Wk for layer in model.enc_layers],\n",
        "            \"enc_Wv\": [layer.mha.Wv for layer in model.enc_layers],\n",
        "            \"enc_Wo\": [layer.mha.Wo for layer in model.enc_layers],\n",
        "            \"enc_ffn_W1\": [layer.ffn.W1 for layer in model.enc_layers],\n",
        "            \"enc_ffn_b1\": [layer.ffn.b1 for layer in model.enc_layers],\n",
        "            \"enc_ffn_W2\": [layer.ffn.W2 for layer in model.enc_layers],\n",
        "            \"enc_ffn_b2\": [layer.ffn.b2 for layer in model.enc_layers],\n",
        "            \"W_out\": model.W_out,\n",
        "            \"b_out\": model.b_out,\n",
        "            \"vocab_size\": model.vocab_size,\n",
        "            \"seq_len\": model.seq_len,\n",
        "            \"d_model\": model.d_model,\n",
        "            \"num_heads\": model.num_heads,\n",
        "            \"d_ff\": model.d_ff,\n",
        "            \"num_layers\": model.num_layers,\n",
        "            \"num_classes\": model.num_classes\n",
        "        },\n",
        "        \"vocab\": vocab\n",
        "    }\n",
        "\n",
        "    with open(path, \"wb\") as f:\n",
        "        pickle.dump(checkpoint, f)\n",
        "    print(f\"Checkpoint saved to {path}\")\n",
        "\n",
        "# Save\n",
        "save_checkpoint(model, vocab)\n",
        "\n",
        "\n",
        "all_preds = []\n",
        "correct = 0\n",
        "for i in range(0, len(X_test), batch_size):\n",
        "    x_batch = X_test[i:i+batch_size]\n",
        "    y_batch = y_test[i:i+batch_size]\n",
        "    preds, _ = predict(model, x_batch)\n",
        "    all_preds.extend(preds)\n",
        "    correct += np.sum(preds == y_batch)\n",
        "\n",
        "accuracy = correct / len(X_test)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8oPdcPJHn69"
      },
      "outputs": [],
      "source": [
        "# ------------------- Phase 6 -------------------\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compare_attention(model, pos_tokens, neg_tokens, vocab, max_len=100):\n",
        "    def get_attn_weights(tokens):\n",
        "        x = np.array([[vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens] +\n",
        "                      [vocab[\"<PAD>\"]] * (max_len - len(tokens))])\n",
        "        x_emb = model.embedding.forward(x) + model.pos_encoding[np.newaxis,:,:]\n",
        "\n",
        "        enc_layer = model.enc_layers[-1]\n",
        "        mha = enc_layer.mha\n",
        "\n",
        "        Q = np.matmul(x_emb, mha.Wq)\n",
        "        K = np.matmul(x_emb, mha.Wk)\n",
        "        V = np.matmul(x_emb, mha.Wv)\n",
        "\n",
        "        batch, seq_len, _ = Q.shape\n",
        "        Qh = Q.reshape(batch, seq_len, mha.num_heads, mha.d_k).transpose(0,2,1,3)\n",
        "        Kh = K.reshape(batch, seq_len, mha.num_heads, mha.d_k).transpose(0,2,1,3)\n",
        "        Vh = V.reshape(batch, seq_len, mha.num_heads, mha.d_k).transpose(0,2,1,3)\n",
        "\n",
        "        attn_weights = []\n",
        "        for h in range(mha.num_heads):\n",
        "            scores = np.matmul(Qh[:,h,:,:], Kh[:,h,:,:].transpose(0,2,1)) / np.sqrt(mha.d_k)\n",
        "            attn_weights.append(softmax(scores[0]))\n",
        "        return attn_weights, min(len(tokens), max_len)\n",
        "\n",
        "    pos_attn, pos_len = get_attn_weights(pos_tokens)\n",
        "    neg_attn, neg_len = get_attn_weights(neg_tokens)\n",
        "\n",
        "    for h in range(len(pos_attn)):\n",
        "        fig, axes = plt.subplots(1,2, figsize=(15,6))\n",
        "        axes[0].imshow(pos_attn[h][:pos_len, :pos_len], cmap='viridis')\n",
        "        axes[0].set_xticks(range(pos_len))\n",
        "        axes[0].set_yticks(range(pos_len))\n",
        "        axes[0].set_xticklabels(pos_tokens[:pos_len], rotation=90)\n",
        "        axes[0].set_yticklabels(pos_tokens[:pos_len])\n",
        "        axes[0].set_title(f\"Head {h+1} Positive\")\n",
        "\n",
        "        axes[1].imshow(neg_attn[h][:neg_len, :neg_len], cmap='viridis')\n",
        "        axes[1].set_xticks(range(neg_len))\n",
        "        axes[1].set_yticks(range(neg_len))\n",
        "        axes[1].set_xticklabels(neg_tokens[:neg_len], rotation=90)\n",
        "        axes[1].set_yticklabels(neg_tokens[:neg_len])\n",
        "        axes[1].set_title(f\"Head {h+1} Negative\")\n",
        "        plt.show()\n",
        "\n",
        "# Example\n",
        "positive_review = \"this product is disgustingly good\".split()\n",
        "negative_review = \"the device works absolutely shit\".split()\n",
        "compare_attention(model, positive_review, negative_review, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8X0BxJiHp9g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Returns confusion matrix for binary classification\n",
        "    [[TP, FN],\n",
        "     [FP, TN]]\n",
        "    \"\"\"\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "    return np.array([[TP, FN],\n",
        "                     [FP, TN]])\n",
        "def classification_metrics(y_true, y_pred):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    TP, FN = cm[0]\n",
        "    FP, TN = cm[1]\n",
        "\n",
        "    accuracy  = (TP + TN) / (TP + TN + FP + FN)\n",
        "    precision = TP / (TP + FP + 1e-12)  # avoid division by zero\n",
        "    recall    = TP / (TP + FN + 1e-12)\n",
        "    f1        = 2 * (precision * recall) / (precision + recall + 1e-12)\n",
        "\n",
        "    return cm, accuracy, precision, recall, f1\n",
        "all_preds = np.array(all_preds)  # from Phase 5\n",
        "y_true = y_test\n",
        "\n",
        "cm, accuracy, precision, recall, f1 = classification_metrics(y_true, all_preds)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(f\"Accuracy : {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall   : {recall:.4f}\")\n",
        "print(f\"F1-score : {f1:.4f}\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.imshow(cm, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.xticks([0,1], [\"Positive\",\"Negative\"])\n",
        "plt.yticks([0,1], [\"Positive\",\"Negative\"])\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        plt.text(j, i, cm[i,j], ha='center', va='center', color='red')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWjGzx8QMSC-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO1lJd5B3oqnHX6foBa+snf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}